# Nirva On-Device Audio Processing Architecture

## ğŸ¯ Complete System Overview

**Mission**: Process necklace recordings locally with 4 on-device AI tasks:
1. **ASR** (Automatic Speech Recognition) - Transcribe audio to text
2. **Speaker Diarization** - "Who spoke when?"
3. **Voiceprint Verification** - Identify if speaker is the user ("self")
4. **Emotion Detection** - Extract emotional state from speech
5. **Event Extraction** - Identify key moments/events

**Current Focus**: Tasks 1-3 (ASR + Diarization + Voiceprint)

---

## ğŸ“Š Current State Analysis

### Existing Components

#### âœ… Cloud-Based (nirva_service)
```python
# nirva_service/nirva/api/voiceprints.py
- Voiceprint enrollment via pyannote.ai
- Cloud-based speaker verification
- S3 storage for audio files
- Embedding extraction (cloud)
```

#### âœ… Flutter App (nirva_app)
```dart
// lib/services/voiceprint_service.dart
- Upload voiceprint audio to S3
- Check voiceprint status
- Download enrolled voiceprint
- Currently: ALL processing in cloud
```

#### âŒ Missing: On-Device Processing
- No local ASR
- No local diarization
- No local voiceprint comparison
- No model switcher (cloud vs on-device)

---

## ğŸ—ï¸ Target Architecture

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NECKLACE RECORDING                        â”‚
â”‚              Stored as audio file on device                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PERIODIC ON-DEVICE PROCESSING                   â”‚
â”‚                  (Batch processing)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼               â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ASR  â”‚      â”‚Diarizationâ”‚   â”‚Voiceprintâ”‚
   â”‚Model â”‚      â”‚  Model    â”‚   â”‚  Model   â”‚
   â””â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚                â”‚              â”‚
      â–¼                â–¼              â–¼
   Transcript    Speaker Segments   User ID
                       â”‚              â”‚
                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Replace speaker  â”‚
                    â”‚ with "self" tag  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Diarized         â”‚
                    â”‚ Transcript       â”‚
                    â”‚ "Self: Hello..." â”‚
                    â”‚ "Person1: Hi..." â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Integration

```
nirva_app (Flutter)
â”œâ”€â”€ ios/
â”‚   â”œâ”€â”€ Runner/
â”‚   â”‚   â”œâ”€â”€ FluidAudioBridge.swift        # ASR + Diarization
â”‚   â”‚   â”œâ”€â”€ CAMPlusVoiceprintBridge.swift # NEW: Voiceprint matching
â”‚   â”‚   â””â”€â”€ Resources/
â”‚   â”‚       â”œâ”€â”€ parakeet_tdt_coreml/      # ASR model
â”‚   â”‚       â”œâ”€â”€ segmentation.mlmodel       # Diarization
â”‚   â”‚       â””â”€â”€ campplus_cn_en_common.mlpackage  # NEW: Voiceprint
â”‚   â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ fluid_audio_service.dart       # ASR + Diarization
â”‚   â”‚   â”œâ”€â”€ voiceprint_verification_service.dart  # NEW: On-device voiceprint
â”‚   â”‚   â”œâ”€â”€ model_switcher_service.dart    # NEW: Cloud vs On-device
â”‚   â”‚   â””â”€â”€ audio_processing_pipeline.dart # NEW: Orchestrator
â”‚   â”‚
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ recordings_page.dart           # MODIFY: Show diarization
â”‚       â”œâ”€â”€ recording_detail_page.dart     # NEW: Per-recording view
â”‚       â””â”€â”€ dev_recording_screen.dart      # NEW: Model switcher UI
â”‚
â””â”€â”€ 3D-Speaker/
    â””â”€â”€ campplus_cn_en_common.mlpackage   # Ready to integrate âœ…
```

---

## ğŸ”„ Processing Pipeline (Detailed)

### Step 1: Offline ASR (Batch Processing)

**Trigger**: Periodically (e.g., every 30 min, or when charging)

```dart
// AudioProcessingPipeline.dart
Future<void> processRecording(String audioFilePath) async {
  // 1. Run ASR (FluidAudio)
  final transcript = await FluidAudioService.transcribe(audioFilePath);

  // transcript = [
  //   {text: "Hello there", start: 0.0, end: 2.1},
  //   {text: "How are you", start: 2.5, end: 4.0},
  // ]
}
```

**Models**:
- FluidAudio (Parakeet TDT v3)
- Output: Raw transcript with timestamps

---

### Step 2: Speaker Diarization

```dart
// AudioProcessingPipeline.dart (continued)
Future<void> processRecording(String audioFilePath) async {
  // ... ASR from Step 1

  // 2. Run Diarization (FluidAudio)
  final diarization = await FluidAudioService.diarize(audioFilePath);

  // diarization = [
  //   {speaker: "Speaker_0", start: 0.0, end: 2.5},
  //   {speaker: "Speaker_1", start: 2.5, end: 5.0},
  //   {speaker: "Speaker_0", start: 5.0, end: 7.0},
  // ]
}
```

**Models**:
- FluidAudio Segmentation
- FluidAudio WeSpeaker (256-dim embeddings)
- Output: Timeline with speaker labels (Speaker_0, Speaker_1, etc.)

---

### Step 3: Voiceprint Verification (NEW - CAM++)

**For each diarized speaker segment:**

```dart
// AudioProcessingPipeline.dart (continued)
Future<void> processRecording(String audioFilePath) async {
  // ... ASR + Diarization from Steps 1-2

  // 3. Load enrolled user voiceprint
  final userEmbedding = await VoiceprintService.getUserEmbedding();

  if (userEmbedding == null) {
    _logger.w('No enrolled voiceprint - skipping speaker verification');
    return diarization; // Return without "self" tagging
  }

  // 4. For each speaker, compare with user voiceprint
  final verifiedDiarization = [];

  for (final segment in diarization) {
    // Extract audio clip for this speaker segment
    final audioClip = extractAudioSegment(
      audioFilePath,
      start: segment.start,
      end: segment.end
    );

    // Extract embedding using CAM++
    final segmentEmbedding = await VoiceprintVerificationService
        .extractEmbedding(audioClip);

    // Compare with user's enrolled voiceprint
    final similarity = cosineSimilarity(userEmbedding, segmentEmbedding);

    // Threshold from EER evaluation (typically 0.5-0.6)
    final isUser = similarity > 0.52;

    verifiedDiarization.add({
      'speaker': isUser ? 'self' : segment.speaker,
      'start': segment.start,
      'end': segment.end,
      'confidence': similarity,
      'original_speaker': segment.speaker, // Keep for UI
    });
  }

  return verifiedDiarization;
}
```

**Models**:
- CAM++ (192-dim embeddings) âœ…
- Cosine similarity comparison
- Output: Speaker labels with "self" tag

**Key Implementation:**

```swift
// ios/Runner/CAMPlusVoiceprintBridge.swift
class CAMPlusVoiceprintBridge: NSObject, FlutterPlugin {
    private let model: MLModel

    func extractEmbedding(audioData: [Float]) -> [Float] {
        // 1. Extract FBank features
        let fbank = extractFBank(audioData) // 80-dim

        // 2. Run CAM++ inference
        let input = campplus_cn_en_commonInput(
            fbank_features: fbank
        )
        let output = try! model.prediction(from: input)

        return Array(output.embedding) // 192-dim
    }

    func compareWithEnrolled(audioClip: [Float]) -> Double {
        let embedding = extractEmbedding(audioClip)
        let enrolledEmbedding = loadEnrolledEmbedding()

        return cosineSimilarity(embedding, enrolledEmbedding)
    }
}
```

---

### Step 4: Merge Transcript + Diarization

```dart
// AudioProcessingPipeline.dart (final)
Future<DiarizedTranscript> processRecording(String audioFilePath) async {
  final transcript = await FluidAudioService.transcribe(audioFilePath);
  final diarization = await diarizeWithVoiceprint(audioFilePath);

  // Merge: Assign transcript words to speakers
  final diarizedTranscript = mergeTranscriptAndDiarization(
    transcript,
    diarization
  );

  // Result:
  // [
  //   {speaker: "self", text: "Hello there", start: 0.0, end: 2.1},
  //   {speaker: "Speaker_1", text: "How are you", start: 2.5, end: 4.0},
  // ]

  return diarizedTranscript;
}
```

---

## ğŸ¨ UI Requirements

### 1. Dev Recording Screen (Model Switcher)

**Purpose**: Easily toggle between cloud vs on-device models for testing

```dart
// lib/pages/dev_recording_screen.dart
class DevRecordingScreen extends StatefulWidget {
  @override
  Widget build(BuildContext context) {
    return Column(
      children: [
        // Model Selection
        Card(
          child: Column(
            children: [
              Text('ASR Model'),
              SegmentedButton(
                segments: [
                  ButtonSegment(value: 'cloud', label: 'Cloud (Deepgram)'),
                  ButtonSegment(value: 'device', label: 'On-Device (Parakeet)'),
                ],
                selected: {_asrModel},
                onSelectionChanged: (Set<String> selection) {
                  setState(() => _asrModel = selection.first);
                },
              ),

              Text('Diarization Model'),
              SegmentedButton(
                segments: [
                  ButtonSegment(value: 'cloud', label: 'Cloud (pyannote.ai)'),
                  ButtonSegment(value: 'device', label: 'On-Device (FluidAudio)'),
                ],
                selected: {_diarizationModel},
                onSelectionChanged: (Set<String> selection) {
                  setState(() => _diarizationModel = selection.first);
                },
              ),

              Text('Voiceprint Model'),
              SegmentedButton(
                segments: [
                  ButtonSegment(value: 'cloud', label: 'Cloud (pyannote.ai)'),
                  ButtonSegment(value: 'device', label: 'On-Device (CAM++)'),
                ],
                selected: {_voiceprintModel},
                onSelectionChanged: (Set<String> selection) {
                  setState(() => _voiceprintModel = selection.first);
                },
              ),
            ],
          ),
        ),

        // Process Button
        ElevatedButton(
          onPressed: () => _processRecording(),
          child: Text('Process Recording'),
        ),

        // Results Display
        _buildResultsView(),
      ],
    );
  }

  Future<void> _processRecording() async {
    final result = await AudioProcessingPipeline.process(
      audioPath: widget.audioPath,
      config: ProcessingConfig(
        asrModel: _asrModel,
        diarizationModel: _diarizationModel,
        voiceprintModel: _voiceprintModel,
      ),
    );

    setState(() => _results = result);
  }
}
```

---

### 2. Recording Detail Page (Diarization View)

```dart
// lib/pages/recording_detail_page.dart
class RecordingDetailPage extends StatelessWidget {
  final DiarizedTranscript transcript;

  @override
  Widget build(BuildContext context) {
    return ListView.builder(
      itemCount: transcript.segments.length,
      itemBuilder: (context, index) {
        final segment = transcript.segments[index];

        return Card(
          color: segment.speaker == 'self'
              ? Colors.blue[50]   // Highlight user's speech
              : Colors.grey[100], // Other speakers
          child: ListTile(
            leading: CircleAvatar(
              child: Text(
                segment.speaker == 'self' ? 'ğŸ‘¤' : 'ğŸ—£ï¸'
              ),
            ),
            title: Text(segment.speaker),
            subtitle: Column(
              crossAxisAlignment: CrossAxisAlignment.start,
              children: [
                Text(segment.text),
                Text(
                  '${segment.start.toStringAsFixed(1)}s - ${segment.end.toStringAsFixed(1)}s',
                  style: TextStyle(fontSize: 12, color: Colors.grey),
                ),
              ],
            ),
            trailing: segment.speaker != 'self' && segment.speaker != 'unknown'
                ? ElevatedButton(
                    onPressed: () => _compareWithVoiceprint(segment),
                    child: Text('Compare'),
                  )
                : null,
          ),
        );
      },
    );
  }

  Future<void> _compareWithVoiceprint(Segment segment) async {
    // Extract audio clip for this segment
    final audioClip = await extractSegmentAudio(
      recording.audioPath,
      segment.start,
      segment.end,
    );

    // Compare with user's voiceprint
    final similarity = await VoiceprintVerificationService
        .compareWithEnrolled(audioClip);

    // Show result
    showDialog(
      context: context,
      builder: (context) => AlertDialog(
        title: Text('Voiceprint Match'),
        content: Column(
          mainAxisSize: MainAxisSize.min,
          children: [
            Text('Similarity: ${(similarity * 100).toStringAsFixed(1)}%'),
            SizedBox(height: 16),
            LinearProgressIndicator(value: similarity),
            SizedBox(height: 16),
            Text(
              similarity > 0.52
                  ? 'âœ… Likely YOU'
                  : 'âŒ Different person',
              style: TextStyle(
                fontSize: 18,
                fontWeight: FontWeight.bold,
                color: similarity > 0.52 ? Colors.green : Colors.red,
              ),
            ),
          ],
        ),
        actions: [
          if (similarity > 0.52)
            TextButton(
              onPressed: () => _markAsSelf(segment),
              child: Text('Mark as Self'),
            ),
          TextButton(
            onPressed: () => Navigator.pop(context),
            child: Text('Close'),
          ),
        ],
      ),
    );
  }
}
```

---

## ğŸ”§ Implementation Plan

### Phase 1: CAM++ Integration (On-Device Voiceprint) âœ… COMPLETE

**Status**: Model converted, ready to integrate

- âœ… Convert CAM++ to CoreML (14MB, 192-dim)
- âœ… Create verification script (Python)
- â­ï¸ Integrate into iOS app

### Phase 2: iOS Bridge (Week 1)

**Create Swift bridges:**

```swift
// CAMPlusVoiceprintBridge.swift (~200 lines)
- Load CAM++ CoreML model
- Extract FBank features
- Run embedding extraction
- Cosine similarity computation
- Caching enrolled embedding

// FlutterMethodChannel
- extractEmbedding(audioPath) -> [Float]
- compareWithEnrolled(audioPath) -> Double
- enrollVoiceprint(audioPath) -> [Float]
```

### Phase 3: Flutter Service Layer (Week 1-2)

```dart
// voiceprint_verification_service.dart
class VoiceprintVerificationService {
  // On-device methods
  static Future<List<double>> extractEmbedding(String audioPath)
  static Future<double> compareWithEnrolled(String audioPath)
  static Future<void> enrollVoiceprint(String audioPath)

  // Hybrid: Store embedding locally + cloud backup
  static Future<void> syncEmbeddingToCloud()
  static Future<List<double>?> getEnrolledEmbedding()
}

// model_switcher_service.dart
class ModelSwitcherService {
  ProcessingConfig config;

  Future<DiarizedTranscript> processRecording(String audioPath) {
    switch (config.voiceprintModel) {
      case 'cloud':
        return _processCloudVoiceprint(audioPath);
      case 'device':
        return _processDeviceVoiceprint(audioPath);
    }
  }
}

// audio_processing_pipeline.dart
class AudioProcessingPipeline {
  static Future<DiarizedTranscript> process({
    required String audioPath,
    required ProcessingConfig config,
  }) async {
    // 1. ASR
    final transcript = await _runASR(audioPath, config.asrModel);

    // 2. Diarization
    final diarization = await _runDiarization(audioPath, config.diarizationModel);

    // 3. Voiceprint verification
    final verified = await _verifyVoiceprints(
      audioPath,
      diarization,
      config.voiceprintModel
    );

    // 4. Merge
    return _mergeDiarizedTranscript(transcript, verified);
  }
}
```

### Phase 4: UI Implementation (Week 2)

1. **Dev Recording Screen**
   - Model switcher controls
   - Process button
   - Results comparison view

2. **Recording Detail Page**
   - Diarized transcript view
   - "Compare" button per speaker
   - Visual distinction for "self"

### Phase 5: Testing & Optimization (Week 3)

1. **Accuracy Testing**
   - Test on real necklace recordings
   - Tune similarity threshold
   - Measure false positive/negative rates

2. **Performance Optimization**
   - Batch processing optimization
   - Embedding caching
   - Background processing

3. **UX Refinement**
   - Loading states
   - Error handling
   - Offline support

---

## ğŸ“Š Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| **ASR Latency** | < 2x real-time | 10 min audio â†’ 20s processing |
| **Diarization** | < 3x real-time | Includes embedding extraction |
| **Voiceprint** | < 0.5s per segment | CAM++ inference |
| **Total Pipeline** | < 5x real-time | Full processing (60 min â†’ 5 min) |
| **Voiceprint EER** | < 2% | False accept/reject rate |
| **Memory** | < 500MB peak | During processing |

---

## ğŸ” Data Privacy

**On-Device Processing Benefits:**
- âœ… No audio uploaded to cloud (stays on device)
- âœ… Voiceprint embedding stored locally
- âœ… Optional cloud backup (encrypted)
- âœ… User controls all processing

**Hybrid Approach:**
- On-device: Default for all processing
- Cloud: Optional fallback for accuracy comparison
- User choice: Toggle in dev screen

---

## ğŸ“ File Structure

```
nirva_app/
â”œâ”€â”€ ios/Runner/
â”‚   â”œâ”€â”€ CAMPlusVoiceprintBridge.swift          # NEW
â”‚   â”œâ”€â”€ FluidAudioBridge.swift                 # MODIFY: Add diarization
â”‚   â”œâ”€â”€ AudioSegmentExtractor.swift            # NEW: Extract clips
â”‚   â””â”€â”€ Resources/
â”‚       â””â”€â”€ campplus_cn_en_common.mlpackage    # NEW: 14MB
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ voiceprint_verification_service.dart   # NEW
â”‚   â”‚   â”œâ”€â”€ model_switcher_service.dart            # NEW
â”‚   â”‚   â”œâ”€â”€ audio_processing_pipeline.dart         # NEW
â”‚   â”‚   â””â”€â”€ fluid_audio_service.dart               # MODIFY
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ diarized_transcript.dart               # NEW
â”‚   â”‚   â”œâ”€â”€ processing_config.dart                 # NEW
â”‚   â”‚   â””â”€â”€ speaker_segment.dart                   # NEW
â”‚   â”‚
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ dev_recording_screen.dart              # NEW
â”‚       â”œâ”€â”€ recording_detail_page.dart             # NEW
â”‚       â””â”€â”€ recordings_page.dart                   # MODIFY
â”‚
â””â”€â”€ 3D-Speaker/
    â”œâ”€â”€ campplus_cn_en_common.mlpackage           # âœ… Ready
    â””â”€â”€ scripts/
        â””â”€â”€ test_speaker_verification.py          # âœ… Testing tool
```

---

## âœ… Next Steps

### Immediate (This Week):
1. Copy CAM++ model to iOS project
2. Create CAMPlusVoiceprintBridge.swift
3. Test embedding extraction on device
4. Create Flutter service wrapper

### Short-term (2-3 Weeks):
1. Implement dev recording screen
2. Build audio processing pipeline
3. Test on real recordings
4. Tune similarity threshold

### Medium-term (1-2 Months):
1. Optimize batch processing
2. Add emotion detection (text-based initially)
3. Implement event extraction
4. Production deployment

---

## ğŸ¯ Success Criteria

1. âœ… On-device voiceprint verification working
2. âœ… "Self" vs "Other" labeling accurate (< 5% error)
3. âœ… Dev screen allows easy model switching
4. âœ… Processing completes in < 5x real-time
5. âœ… UI clearly shows diarization results
6. âœ… User can manually verify/correct speaker labels

---

**Ready to start implementation?**

The CAM++ model is ready, we have the architecture mapped out, and the integration path is clear. Let me know which phase you want to tackle first!
